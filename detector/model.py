import numpy as np
import cv2
import json
import os
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import seaborn as sns
from collections import Counter

import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Conv2D, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D, Input, Concatenate
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras.backend as K

# –§–∏–∫—Å–∏—Ä—É–µ–º —Å–µ–º–µ–Ω–∞
np.random.seed(42)
tf.random.set_seed(42)

def parse_coco_annotations(annotations_file, images_dir):
    """
    –ü–∞—Ä—Å–∏—Ç COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç bounding boxes.
    –¢–∞–∫–∂–µ –Ω–∞—Ö–æ–¥–∏—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (—Å—á–∏—Ç–∞–µ—Ç –∏—Ö –∫–∞–∫ "–±–µ–∑ –∫—Ä—É–∂–µ–∫")
    """
    print("üìã –ü–ê–†–°–ò–ù–ì COCO –ê–ù–ù–û–¢–ê–¶–ò–ô –ò –ü–û–ò–°–ö –í–°–ï–• –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º COCO –¥–∞–Ω–Ω—ã–µ
    with open(annotations_file, 'r') as f:
        coco_data = json.load(f)
    
    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
    images_dict = {img['id']: img for img in coco_data['images']}
    coco_filenames = {img['file_name'] for img in coco_data['images']}
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –ø–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
    annotations_by_image = {}
    for ann in coco_data['annotations']:
        image_id = ann['image_id']
        if image_id not in annotations_by_image:
            annotations_by_image[image_id] = []
        annotations_by_image[image_id].append(ann)
    
    # –ù–∞—Ö–æ–¥–∏–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ
    all_image_files = set()
    for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp']:
        files = glob(os.path.join(images_dir, ext))
        files.extend(glob(os.path.join(images_dir, ext.upper())))
        for f in files:
            all_image_files.add(os.path.basename(f))
    
    print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ: {len(all_image_files)}")
    print(f"–§–∞–π–ª–æ–≤ –≤ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è—Ö: {len(coco_filenames)}")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    image_data = []
    
    # –°–Ω–∞—á–∞–ª–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –° –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
    for image_id, img_info in images_dict.items():
        file_path = os.path.join(images_dir, img_info['file_name'])
        
        if not os.path.exists(file_path):
            continue
            
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        img_width = img_info['width']
        img_height = img_info['height']
        
        if image_id in annotations_by_image:
            # –ï—Å—Ç—å –∫—Ä—É–∂–∫–∏ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∞–Ω–Ω–æ—Ç–∞—Ü–∏—é
            ann = annotations_by_image[image_id][0]
            bbox = ann['bbox']  # [x, y, width, height]
            
            # COCO —Ñ–æ—Ä–º–∞—Ç: [x_top_left, y_top_left, width, height]
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            x = bbox[0] / img_width
            y = bbox[1] / img_height
            w = bbox[2] / img_width
            h = bbox[3] / img_height
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            center_x = x + w / 2
            center_y = y + h / 2
            
            image_data.append({
                'file_name': img_info['file_name'],
                'file_path': file_path,
                'has_cup': 1,
                'bbox': [center_x, center_y, w, h],
                'original_size': [img_width, img_height]
            })
    
    # –¢–µ–ø–µ—Ä—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ë–ï–ó –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    files_without_annotations = all_image_files - coco_filenames
    print(f"–§–∞–π–ª–æ–≤ –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π (—Å—á–∏—Ç–∞–µ–º –∫–∞–∫ '–±–µ–∑ –∫—Ä—É–∂–µ–∫'): {len(files_without_annotations)}")
    
    for filename in files_without_annotations:
        file_path = os.path.join(images_dir, filename)
        
        if os.path.exists(file_path):
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                img = cv2.imread(file_path)
                if img is not None:
                    img_height, img_width = img.shape[:2]
                    
                    image_data.append({
                        'file_name': filename,
                        'file_path': file_path,
                        'has_cup': 0,
                        'bbox': [0.0, 0.0, 0.0, 0.0],  # –ù—É–ª–µ–≤–æ–π bbox –¥–ª—è "–Ω–µ—Ç –æ–±—ä–µ–∫—Ç–∞"
                        'original_size': [img_width, img_height]
                    })
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {filename}: {e}")
                continue
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    with_cups = sum(1 for item in image_data if item['has_cup'] == 1)
    without_cups = len(image_data) - with_cups
    
    print(f"\nüìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(image_data)}")
    print(f"–° –∫—Ä—É–∂–∫–∞–º–∏ (–∏–∑ COCO): {with_cups}")
    print(f"–ë–µ–∑ –∫—Ä—É–∂–µ–∫ (–±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π): {without_cups}")
    
    if without_cups == 0:
        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∫—Ä—É–∂–µ–∫!")
        print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ –ø–∞–ø–∫–µ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    
    return image_data

def enhanced_preprocessing_detection(image_path, target_size=(224, 224)):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
    """
    try:
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = cv2.imread(image_path)
        if image is None:
            return None
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        h, w = image.shape[:2]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
        scale = min(target_size[0] / h, target_size[1] / w)
        new_h = int(h * scale)
        new_w = int(w * scale)
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å padding
        final_image = np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        y_offset = (target_size[0] - new_h) // 2
        x_offset = (target_size[1] - new_w) // 2
        final_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è ImageNet
        normalized = final_image.astype("float32") / 255.0
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        normalized = (normalized - mean) / std
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–∞–∫–∂–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–ª—è –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ bbox
        return normalized, scale, x_offset, y_offset
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
        return None

def adjust_bbox_for_preprocessing(bbox, original_size, scale, x_offset, y_offset, target_size):
    """
    –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ—Ç –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    """
    if bbox == [0.0, 0.0, 0.0, 0.0]:
        return bbox
    
    orig_w, orig_h = original_size
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º bbox –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º
    center_x = bbox[0] * orig_w
    center_y = bbox[1] * orig_h
    width = bbox[2] * orig_w
    height = bbox[3] * orig_h
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
    center_x = center_x * scale + x_offset
    center_y = center_y * scale + y_offset
    width = width * scale
    height = height * scale
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ —Ä–∞–∑–º–µ—Ä—É target_size
    center_x = center_x / target_size[1]
    center_y = center_y / target_size[0]
    width = width / target_size[1]
    height = height / target_size[0]
    
    return [center_x, center_y, width, height]

def create_detection_dataset(images_dir="train", annotations_file="train/_annotations.coco.json", 
                           target_size=(224, 224), max_samples=None):
    """
    –°–æ–∑–¥–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤
    """
    print("üéØ –°–û–ó–î–ê–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò")
    print("="*50)
    
    # –ü–∞—Ä—Å–∏–º –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    image_data = parse_coco_annotations(annotations_file, images_dir)
    
    if max_samples:
        # –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–±–æ—Ä –æ–±—Ä–∞–∑—Ü–æ–≤
        with_cups = [item for item in image_data if item['has_cup'] == 1]
        without_cups = [item for item in image_data if item['has_cup'] == 0]
        
        print(f"–î–æ—Å—Ç—É–ø–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫—Ä—É–∂–∫–∞–º–∏: {len(with_cups)}")
        print(f"–î–æ—Å—Ç—É–ø–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∫—Ä—É–∂–µ–∫: {len(without_cups)}")
        
        if len(without_cups) == 0:
            print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∫—Ä—É–∂–µ–∫!")
            print("   –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –∫—Ä—É–∂–∫–∞–º–∏")
            samples_per_class = min(max_samples, len(with_cups))
            image_data = with_cups[:samples_per_class]
        else:
            # –ë–µ—Ä–µ–º –ø–æ—Ä–æ–≤–Ω—É –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            samples_per_class = min(max_samples // 2, len(with_cups), len(without_cups))
            selected_with_cups = with_cups[:samples_per_class]
            selected_without_cups = without_cups[:samples_per_class]
            image_data = selected_with_cups + selected_without_cups
            
            print(f"–í—ã–±—Ä–∞–Ω–æ —Å –∫—Ä—É–∂–∫–∞–º–∏: {len(selected_with_cups)}")
            print(f"–í—ã–±—Ä–∞–Ω–æ –±–µ–∑ –∫—Ä—É–∂–µ–∫: {len(selected_without_cups)}")
    
    print(f"–ò—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º: {len(image_data)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    images = []
    labels = []  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–µ—Å—Ç—å –∫—Ä—É–∂–∫–∞ –∏–ª–∏ –Ω–µ—Ç)
    bboxes = []  # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bounding box
    
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
    
    for i, item in enumerate(image_data):
        if i % 50 == 0:
            print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(image_data)}")
        
        result = enhanced_preprocessing_detection(item['file_path'], target_size)
        if result is not None:
            processed_image, scale, x_offset, y_offset = result
            
            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º bbox –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã
            adjusted_bbox = adjust_bbox_for_preprocessing(
                item['bbox'], item['original_size'], 
                scale, x_offset, y_offset, target_size
            )
            
            images.append(processed_image)
            labels.append(item['has_cup'])
            bboxes.append(adjusted_bbox)
    
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã
    images = np.array(images, dtype="float32")
    labels = np.array(labels, dtype="float32")
    bboxes = np.array(bboxes, dtype="float32")
    
    print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {Counter(labels)}")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {images.shape}")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö - –º–µ—Ç–∫–∏: {labels.shape}")
    print(f"–§–æ—Ä–º–∞ –¥–∞–Ω–Ω—ã—Ö - bbox: {bboxes.shape}")
    
    return images, labels, bboxes

def create_detection_model(input_shape=(224, 224, 3)):
    """
    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –æ–±—ä–µ–∫—Ç–æ–≤ —Å –¥–≤—É–º—è –≤—ã—Ö–æ–¥–∞–º–∏:
    1. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–µ—Å—Ç—å –æ–±—ä–µ–∫—Ç –∏–ª–∏ –Ω–µ—Ç)
    2. –†–µ–≥—Ä–µ—Å—Å–∏—è bbox –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
    """
    print("\nüß† –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –î–ï–¢–ï–ö–¶–ò–ò")
    print("="*50)
    
    # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
    base_model = MobileNetV2(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet'
    )
    base_model.trainable = False
    
    # –í—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
    inputs = Input(shape=input_shape)
    
    # Backbone
    x = base_model(inputs, training=False)
    x = GlobalAveragePooling2D()(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    
    # –û–±—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    features = Dense(512, activation='relu', name='features')(x)
    features = BatchNormalization()(features)
    features = Dropout(0.3)(features)
    
    # –í–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    classification = Dense(256, activation='relu', name='cls_dense')(features)
    classification = BatchNormalization()(classification)
    classification = Dropout(0.2)(classification)
    classification_output = Dense(1, activation='sigmoid', name='classification')(classification)
    
    # –í–µ—Ç–∫–∞ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ bbox
    regression = Dense(256, activation='relu', name='reg_dense')(features)
    regression = BatchNormalization()(regression)
    regression = Dropout(0.2)(regression)
    bbox_output = Dense(4, activation='sigmoid', name='bbox_regression')(regression)  # [center_x, center_y, width, height]
    
    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = Model(inputs=inputs, outputs=[classification_output, bbox_output])
    
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
    
    return model

def detection_loss(y_true_cls, y_pred_cls, y_true_bbox, y_pred_bbox):
    """
    –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
    """
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è
    cls_loss = tf.keras.losses.binary_crossentropy(y_true_cls, y_pred_cls)
    
    # –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–∞—è –ø–æ—Ç–µ—Ä—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –æ–±—ä–µ–∫—Ç–∞–º–∏)
    mask = tf.cast(y_true_cls > 0.5, tf.float32)  # –ú–∞—Å–∫–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –æ–±—ä–µ–∫—Ç–∞–º–∏
    
    # Smooth L1 loss –¥–ª—è bbox
    diff = tf.abs(y_true_bbox - y_pred_bbox)
    smooth_l1 = tf.where(
        diff < 1.0,
        0.5 * tf.square(diff),
        diff - 0.5
    )
    
    bbox_loss = tf.reduce_mean(smooth_l1, axis=1)
    bbox_loss = bbox_loss * mask  # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞—Å–∫—É
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–æ—Ç–µ—Ä–∏
    total_loss = cls_loss + bbox_loss
    
    return total_loss

def custom_detection_loss():
    """
    –ö–∞—Å—Ç–æ–º–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–æ–¥–µ–ª–∏ —Å –¥–≤—É–º—è –≤—ã—Ö–æ–¥–∞–º–∏
    """
    def loss_function(y_true, y_pred):
        y_true_cls = y_true[0]
        y_true_bbox = y_true[1]
        y_pred_cls = y_pred[0]
        y_pred_bbox = y_pred[1]
        
        return detection_loss(y_true_cls, y_pred_cls, y_true_bbox, y_pred_bbox)
    
    return loss_function

def train_detection_model():
    """
    –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏
    """
    print("üöÄ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–ï–¢–ï–ö–¶–ò–ò")
    print("="*70)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        images, labels, bboxes = create_detection_dataset(
            images_dir="train",
            annotations_file="train/_annotations.coco.json",
            target_size=(224, 224),
            max_samples=300  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        )
        
        if len(images) < 10:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        if len(np.unique(labels)) > 1:
            # –ï—Å—Ç—å –æ–±–∞ –∫–ª–∞—Å—Å–∞ - –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é
            X_train, X_test, y_cls_train, y_cls_test, y_bbox_train, y_bbox_test = train_test_split(
                images, labels, bboxes, test_size=0.2, random_state=42, stratify=labels
            )
            
            X_train, X_val, y_cls_train, y_cls_val, y_bbox_train, y_bbox_val = train_test_split(
                X_train, y_cls_train, y_bbox_train, test_size=0.2, random_state=42, stratify=y_cls_train
            )
        else:
            # –¢–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å - –±–µ–∑ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
            print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å, —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –±–µ–∑ —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏")
            X_train, X_test, y_cls_train, y_cls_test, y_bbox_train, y_bbox_test = train_test_split(
                images, labels, bboxes, test_size=0.2, random_state=42
            )
            
            X_train, X_val, y_cls_train, y_cls_val, y_bbox_train, y_bbox_val = train_test_split(
                X_train, y_cls_train, y_bbox_train, test_size=0.2, random_state=42
            )
        
        print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"  –û–±—É—á–µ–Ω–∏–µ: {X_train.shape[0]} ({Counter(y_cls_train)})")
        print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è: {X_val.shape[0]} ({Counter(y_cls_val)})")
        print(f"  –¢–µ—Å—Ç: {X_test.shape[0]} ({Counter(y_cls_test)})")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        unique_classes = np.unique(labels)
        if len(unique_classes) == 1:
            if unique_classes[0] == 1:
                print("\n‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–º–µ—é—Ç –∫—Ä—É–∂–∫–∏!")
                print("   –ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è —Ä–∞–∑–ª–∏—á–∞—Ç—å '–µ—Å—Ç—å –∫—Ä—É–∂–∫–∞' –∏ '–Ω–µ—Ç –∫—Ä—É–∂–∫–∏'")
                print("   –î–æ–±–∞–≤—å—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–µ–∑ –∫—Ä—É–∂–µ–∫ –≤ –ø–∞–ø–∫—É –∏–ª–∏ —É–±–µ—Ä–∏—Ç–µ –∏—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏")
            else:
                print("\n‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ë–ï–ó –∫—Ä—É–∂–µ–∫!")
                print("   –ú–æ–¥–µ–ª—å –Ω–µ —Å–º–æ–∂–µ—Ç –Ω–∞—É—á–∏—Ç—å—Å—è –Ω–∞—Ö–æ–¥–∏—Ç—å –∫—Ä—É–∂–∫–∏")
            
            print("\nüí° –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –ø–ª–æ—Ö–∏–º–∏...")
        
        elif len(unique_classes) == 2:
            class_counts = Counter(labels)
            ratio = min(class_counts.values()) / max(class_counts.values())
            if ratio < 0.3:
                print(f"\n‚ö†Ô∏è –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í: –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ {ratio:.2f}")
                print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç")
            else:
                print(f"\n‚úÖ –•–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤: {class_counts}")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        model = create_detection_model()
        
        # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
        model.compile(
            optimizer=Adam(learning_rate=0.0001),
            loss={
                'classification': 'binary_crossentropy',
                'bbox_regression': 'mse'
            },
            loss_weights={
                'classification': 1.0,
                'bbox_regression': 1.0
            },
            metrics={
                'classification': ['accuracy'],
                'bbox_regression': ['mae']
            }
        )
        
        # Callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_classification_accuracy',
                patience=15,
                restore_best_weights=True,
                verbose=1
            ),
            ModelCheckpoint(
                filepath='cup_detection_model.h5',
                monitor='val_classification_accuracy',
                mode='max',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=7,
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        # –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è (–æ—Å—Ç–æ—Ä–æ–∂–Ω–∞—è –¥–ª—è bbox)
        datagen = ImageDataGenerator(
            rotation_range=10,
            width_shift_range=0.1,
            height_shift_range=0.1,
            zoom_range=0.1,
            horizontal_flip=True,
            fill_mode='nearest'
        )
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        epochs = 50
        batch_size = 16
        
        print(f"\nüéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        print(f"  –≠–ø–æ—Ö–∏: {epochs}")
        print(f"  Batch size: {batch_size}")
        print(f"  Learning rate: 0.0001")
        
        # –û–±—É—á–µ–Ω–∏–µ
        print(f"\n{'='*50}")
        print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï")
        print(f"{'='*50}")
        
        history = model.fit(
            X_train,
            {'classification': y_cls_train, 'bbox_regression': y_bbox_train},
            validation_data=(X_val, {'classification': y_cls_val, 'bbox_regression': y_bbox_val}),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à–∏–µ –≤–µ—Å–∞
        try:
            model.load_weights('cup_detection_model.h5')
            print("\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ª—É—á—à–∏–µ –≤–µ—Å–∞")
        except:
            print("\n‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞")
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        print("\nüìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò")
        print("="*40)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        predictions = model.predict(X_test, verbose=0)
        cls_pred = predictions[0].squeeze()
        bbox_pred = predictions[1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        cls_pred_binary = (cls_pred > 0.5).astype(int)
        cls_accuracy = np.mean(cls_pred_binary == y_cls_test)
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {cls_accuracy:.4f}")
        
        # –ú–µ—Ç—Ä–∏–∫–∏ bbox (—Ç–æ–ª—å–∫–æ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –æ–±—ä–µ–∫—Ç–∞–º–∏)
        has_object_mask = y_cls_test == 1
        if np.sum(has_object_mask) > 0:
            bbox_mae = mean_absolute_error(
                y_bbox_test[has_object_mask], 
                bbox_pred[has_object_mask]
            )
            bbox_mse = mean_squared_error(
                y_bbox_test[has_object_mask], 
                bbox_pred[has_object_mask]
            )
            
            print(f"Bbox MAE: {bbox_mae:.4f}")
            print(f"Bbox MSE: {bbox_mse:.4f}")
        
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        visualize_detection_results(X_test[:8], y_cls_test[:8], y_bbox_test[:8], 
                                  cls_pred[:8], bbox_pred[:8])
        
        # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
        plot_training_history(history)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        try:
            model.save('cup_detection_model_final.h5')
            print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: cup_detection_model_final.h5")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        
        print(f"\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {cls_accuracy:.4f}")
        if np.sum(has_object_mask) > 0:
            print(f"–¢–æ—á–Ω–æ—Å—Ç—å bbox (MAE): {bbox_mae:.4f}")
        
        return model, history, cls_accuracy
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
        import traceback
        traceback.print_exc()
        return None, None, 0

def visualize_detection_results(images, true_cls, true_bbox, pred_cls, pred_bbox):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏
    """
    print("\nüñºÔ∏è –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í –î–ï–¢–ï–ö–¶–ò–ò")
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    def denormalize_imagenet(img):
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        denorm = img * std + mean
        return np.clip(denorm, 0, 1)
    
    # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–∏—Å–æ–≤–∞–Ω–∏—è bbox
    def draw_bbox(ax, bbox, color, label):
        center_x, center_y, width, height = bbox
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ corner coordinates
        x = center_x - width / 2
        y = center_y - height / 2
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫ —Ä–∞–∑–º–µ—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (224x224)
        x *= 224
        y *= 224
        width *= 224
        height *= 224
        
        rect = patches.Rectangle((x, y), width, height, 
                               linewidth=2, edgecolor=color, 
                               facecolor='none', label=label)
        ax.add_patch(rect)
    
    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.flatten()
    
    for i in range(min(8, len(images))):
        img = denormalize_imagenet(images[i])
        
        axes[i].imshow(img)
        axes[i].axis('off')
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        true_label = "Cup" if true_cls[i] > 0.5 else "No Cup"
        pred_label = "Cup" if pred_cls[i] > 0.5 else "No Cup"
        confidence = pred_cls[i]
        
        title = f"True: {true_label}\nPred: {pred_label} ({confidence:.3f})"
        axes[i].set_title(title, fontsize=10)
        
        # –†–∏—Å—É–µ–º bbox –µ—Å–ª–∏ –µ—Å—Ç—å –æ–±—ä–µ–∫—Ç
        if true_cls[i] > 0.5:
            draw_bbox(axes[i], true_bbox[i], 'green', 'True')
        
        if pred_cls[i] > 0.5:
            draw_bbox(axes[i], pred_bbox[i], 'red', 'Pred')
        
        # –õ–µ–≥–µ–Ω–¥–∞
        if i == 0:
            axes[i].legend(loc='upper right', fontsize=8)
    
    plt.tight_layout()
    plt.show()

def plot_training_history(history):
    """
    –ì—Ä–∞—Ñ–∏–∫ –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è
    """
    plt.figure(figsize=(15, 5))
    
    # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    plt.subplot(1, 3, 1)
    plt.plot(history.history['classification_accuracy'], label='Train')
    plt.plot(history.history['val_classification_accuracy'], label='Validation')
    plt.title('Classification Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    plt.subplot(1, 3, 2)
    plt.plot(history.history['loss'], label='Train')
    plt.plot(history.history['val_loss'], label='Validation')
    plt.title('Total Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    
    # –ì—Ä–∞—Ñ–∏–∫ bbox MAE
    plt.subplot(1, 3, 3)
    plt.plot(history.history['bbox_regression_mae'], label='Train')
    plt.plot(history.history['val_bbox_regression_mae'], label='Validation')
    plt.title('Bbox MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    plt.legend()
    
    plt.tight_layout()
    plt.show()

def test_detection_model(model_path='cup_detection_model_final.h5', 
                        test_image_path=None):
    """
    –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    """
    print("üß™ –¢–ï–°–¢ –ú–û–î–ï–õ–ò –î–ï–¢–ï–ö–¶–ò–ò")
    print("="*40)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = tf.keras.models.load_model(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        
        if test_image_path and os.path.exists(test_image_path):
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            result = enhanced_preprocessing_detection(test_image_path)
            if result is not None:
                processed_image, scale, x_offset, y_offset = result
                
                # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                predictions = model.predict(np.expand_dims(processed_image, axis=0), verbose=0)
                cls_pred = predictions[0][0][0]
                bbox_pred = predictions[1][0]
                
                print(f"\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
                print(f"  –§–∞–π–ª: {os.path.basename(test_image_path)}")
                print(f"  –ï—Å—Ç—å –∫—Ä—É–∂–∫–∞: {'–î–∞' if cls_pred > 0.5 else '–ù–µ—Ç'} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {cls_pred:.3f})")
                
                if cls_pred > 0.5:
                    center_x, center_y, width, height = bbox_pred
                    print(f"  –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã bbox:")
                    print(f"    –¶–µ–Ω—Ç—Ä X: {center_x:.3f}")
                    print(f"    –¶–µ–Ω—Ç—Ä Y: {center_y:.3f}")
                    print(f"    –®–∏—Ä–∏–Ω–∞: {width:.3f}")
                    print(f"    –í—ã—Å–æ—Ç–∞: {height:.3f}")
                    
                    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    visualize_single_detection(test_image_path, cls_pred, bbox_pred)
            else:
                print("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        else:
            print("‚ö†Ô∏è –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

def visualize_single_detection(image_path, cls_pred, bbox_pred):
    """
    –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –Ω–∞ –æ–¥–Ω–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
    """
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        result = enhanced_preprocessing_detection(image_path)
        if result is None:
            return
        
        processed_image, scale, x_offset, y_offset = result
        
        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        denorm = processed_image * std + mean
        denorm = np.clip(denorm, 0, 1)
        
        plt.figure(figsize=(10, 8))
        plt.imshow(denorm)
        
        # –†–∏—Å—É–µ–º bbox –µ—Å–ª–∏ –æ–±—ä–µ–∫—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω
        if cls_pred > 0.5:
            center_x, center_y, width, height = bbox_pred
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ corner coordinates
            x = center_x - width / 2
            y = center_y - height / 2
            
            # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –∫ —Ä–∞–∑–º–µ—Ä—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            x *= 224
            y *= 224
            width *= 224
            height *= 224
            
            rect = patches.Rectangle((x, y), width, height, 
                                   linewidth=3, edgecolor='red', 
                                   facecolor='none')
            plt.gca().add_patch(rect)
            
            plt.text(x, y-5, f'Cup: {cls_pred:.3f}', 
                    bbox=dict(boxstyle="round,pad=0.3", facecolor="red", alpha=0.7),
                    fontsize=12, color='white')
        
        plt.title(f'Detection Result\nConfidence: {cls_pred:.3f}')
        plt.axis('off')
        plt.show()
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")

if __name__ == "__main__":
    print("üéØ –î–ï–¢–ï–ö–¶–ò–Ø –ö–†–£–ñ–ï–ö –° –ö–û–û–†–î–ò–ù–ê–¢–ê–ú–ò BBOX")
    print("="*70)
    print("–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
    print("‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (–µ—Å—Ç—å –∫—Ä—É–∂–∫–∞ –∏–ª–∏ –Ω–µ—Ç)")
    print("‚úÖ –î–µ—Ç–µ–∫—Ü–∏—è —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ bbox")
    print("‚úÖ Transfer Learning —Å MobileNetV2")
    print("‚úÖ –î–≤–æ–π–Ω–æ–π –≤—ã—Ö–æ–¥: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è + —Ä–µ–≥—Ä–µ—Å—Å–∏—è")
    print("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ COCO –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    print("="*70)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    print(f"\n{'='*50}")
    input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è...")
    
    model, history, accuracy = train_detection_model()
    
    if accuracy > 0.7:
        print(f"\n{'='*50}")
        print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
        
        # –ü—Ä–∏–º–µ—Ä —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å)
        # test_detection_model('cup_detection_model_final.h5', 'path/to/test/image.jpg')
        
    print(f"\nüèÅ –ó–ê–í–ï–†–®–ï–ù–û!")
    if accuracy > 0.8:
        print("üéâ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    elif accuracy > 0.7:
        print("‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    else:
        print("‚ùå –ù—É–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è...")
    
    print("\nüí° –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å:")
    print("1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å: tf.keras.models.load_model('cup_detection_model_final.h5')")
    print("2. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–µ–π enhanced_preprocessing_detection()")
    print("3. –ü–æ–ª—É—á–∏—Ç–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: model.predict(image)")
    print("4. –†–µ–∑—É–ª—å—Ç–∞—Ç: [classification_confidence, [center_x, center_y, width, height]]")

