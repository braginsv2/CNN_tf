import numpy as np
import cv2
import json
import os
import matplotlib.pyplot as plt
from glob import glob
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from collections import Counter
import tensorflow as tf
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.image import ImageDataGenerator


# –§–∏–∫—Å–∏—Ä—É–µ–º —Å–µ–º–µ–Ω–∞
np.random.seed(42)
tf.random.set_seed(42)


def create_transfer_learning_model(input_shape=(224, 224, 3), num_classes=2):
    """
    –°–æ–∑–¥–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ MobileNetV2 (transfer learning)
    """
    print("\nüß† –°–û–ó–î–ê–ù–ò–ï –ú–û–î–ï–õ–ò –° TRANSFER LEARNING")
    print("="*50)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    base_model = MobileNetV2(
        input_shape=input_shape,
        include_top=False,
        weights='imagenet'
    )
    
    # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
    base_model.trainable = False
    
    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—à–∏ —Å–ª–æ–∏
    model = Sequential([
        base_model,
        GlobalAveragePooling2D(),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.3),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    
    # –ö–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Å –Ω–∏–∑–∫–∏–º learning rate
    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    print(f"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: MobileNetV2")
    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {model.count_params():,}")
    print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}")
    
    return model

def enhanced_preprocessing(image_path, target_size=(224, 224)):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –¥–ª—è ImageNet
    """
    try:
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = cv2.imread(image_path)
        if image is None:
            return None
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ RGB
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø–æ—Ä—Ü–∏–π
        h, w = image.shape[:2]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤—ã–µ —Ä–∞–∑–º–µ—Ä—ã
        if h > w:
            new_h = target_size[0]
            new_w = int(w * target_size[0] / h)
        else:
            new_w = target_size[1]
            new_h = int(h * target_size[1] / w)
        
        # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä
        resized = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å padding
        final_image = np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8)
        
        # –¶–µ–Ω—Ç—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        y_offset = (target_size[0] - new_h) // 2
        x_offset = (target_size[1] - new_w) // 2
        final_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = resized
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è ImageNet (–≤–∞–∂–Ω–æ –¥–ª—è transfer learning!)
        normalized = final_image.astype("float32") / 255.0
        
        # ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        normalized = (normalized - mean) / std
        
        return normalized
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {image_path}: {e}")
        return None

def create_contrast_enhanced_dataset(images_dir="train", annotations_file="train/_annotations.coco.json", 
                                   target_size=(224, 224), samples_per_class=150):
    """
    –°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    """
    print("üéØ –°–û–ó–î–ê–ù–ò–ï –ö–û–ù–¢–†–ê–°–¢–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("="*50)
    
    # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    files_with_cups, files_without_cups = deep_data_analysis(images_dir, annotations_file)
    
    print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ {samples_per_class} –æ–±—Ä–∞–∑—Ü–æ–≤ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞")
    
    # –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º –æ–±—Ä–∞–∑—Ü—ã
    cup_files = list(files_with_cups)[:samples_per_class]
    no_cup_files = list(files_without_cups)[:samples_per_class]
    
    data = []
    labels = []
    filenames = []
    
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫—Ä—É–∂–∫–∞–º–∏...")
    successful_cups = 0
    for i, filename in enumerate(cup_files):
        if i % 25 == 0:
            print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(cup_files)}")
        
        image_path = os.path.join(images_dir, filename)
        if os.path.exists(image_path):
            processed_image = enhanced_preprocessing(image_path, target_size)
            if processed_image is not None:
                data.append(processed_image)
                labels.append(1)  # cup
                filenames.append(filename)
                successful_cups += 1
    
    print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∫—Ä—É–∂–µ–∫...")
    successful_no_cups = 0
    for i, filename in enumerate(no_cup_files):
        if i % 25 == 0:
            print(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i}/{len(no_cup_files)}")
        
        image_path = os.path.join(images_dir, filename)
        if os.path.exists(image_path):
            processed_image = enhanced_preprocessing(image_path, target_size)
            if processed_image is not None:
                data.append(processed_image)
                labels.append(0)  # no cup
                filenames.append(filename)
                successful_no_cups += 1
    
    print(f"\n‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ:")
    print(f"  –° –∫—Ä—É–∂–∫–∞–º–∏: {successful_cups}")
    print(f"  –ë–µ–∑ –∫—Ä—É–∂–µ–∫: {successful_no_cups}")
    print(f"  –ò—Ç–æ–≥–æ: {len(data)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    if len(data) == 0:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è!")
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã
    data = np.array(data, dtype="float32")
    labels = np.array(labels)
    
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {Counter(labels)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  –§–æ—Ä–º–∞: {data.shape}")
    print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{data.min():.3f}, {data.max():.3f}]")
    print(f"  –°—Ä–µ–¥–Ω–µ–µ: {data.mean():.3f}")
    print(f"  –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {data.std():.3f}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏
    cup_data = data[labels == 1]
    no_cup_data = data[labels == 0]
    
    if len(cup_data) > 0 and len(no_cup_data) > 0:
        cup_mean = np.mean(cup_data)
        no_cup_mean = np.mean(no_cup_data)
        difference = abs(cup_mean - no_cup_mean)
        
        print(f"\nüîç –†–∞–∑–ª–∏—á–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏:")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –¥–ª—è –∫—Ä—É–∂–µ–∫: {cup_mean:.4f}")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ –¥–ª—è –Ω–µ-–∫—Ä—É–∂–µ–∫: {no_cup_mean:.4f}")
        print(f"  –ê–±—Å–æ–ª—é—Ç–Ω–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å: {difference:.4f}")
        
        if difference > 0.1:
            print("‚úÖ –ö–ª–∞—Å—Å—ã —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–ª–∏—á–∏–º—ã!")
        elif difference > 0.05:
            print("‚ö†Ô∏è –ö–ª–∞—Å—Å—ã —É–º–µ—Ä–µ–Ω–Ω–æ —Ä–∞–∑–ª–∏—á–∏–º—ã")
        else:
            print("‚ùå –ö–ª–∞—Å—Å—ã –ø–ª–æ—Ö–æ —Ä–∞–∑–ª–∏—á–∏–º—ã!")
    
    return data, labels, filenames

def show_sample_images_detailed(data, labels, filenames, num_samples=8):
    """
    –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—Ä–∞–∑—Ü–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º
    """
    print(f"\nüñºÔ∏è –î–ï–¢–ê–õ–¨–ù–ê–Ø –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –û–ë–†–ê–ó–¶–û–í")
    print("="*50)
    
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ–±—Ä–∞—Ç–Ω–∞—è ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    def denormalize_imagenet(img):
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        denorm = img * std + mean
        return np.clip(denorm, 0, 1)
    
    cup_indices = np.where(labels == 1)[0]
    no_cup_indices = np.where(labels == 0)[0]
    
    fig, axes = plt.subplots(3, num_samples//2, figsize=(16, 8))
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä—É–∂–∫–∏
    print("–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –∫—Ä—É–∂–∫–∞–º–∏:")
    for i in range(num_samples//2):
        if i < len(cup_indices):
            idx = cup_indices[i]
            img = denormalize_imagenet(data[idx])
            axes[0, i].imshow(img)
            axes[0, i].set_title(f'Cup: {filenames[idx][:12]}...\nMean: {np.mean(data[idx]):.3f}', fontsize=8)
            axes[0, i].axis('off')
            
            print(f"  {i+1}. {filenames[idx]}: mean={np.mean(data[idx]):.3f}, std={np.std(data[idx]):.3f}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–µ-–∫—Ä—É–∂–∫–∏
    print("\n–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –±–µ–∑ –∫—Ä—É–∂–µ–∫:")
    for i in range(num_samples//2):
        if i < len(no_cup_indices):
            idx = no_cup_indices[i]
            img = denormalize_imagenet(data[idx])
            axes[1, i].imshow(img)
            axes[1, i].set_title(f'No Cup: {filenames[idx][:12]}...\nMean: {np.mean(data[idx]):.3f}', fontsize=8)
            axes[1, i].axis('off')
            
            print(f"  {i+1}. {filenames[idx]}: mean={np.mean(data[idx]):.3f}, std={np.std(data[idx]):.3f}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–Ω–æ—Å—Ç–∏
    for i in range(num_samples//2):
        if i < len(cup_indices) and i < len(no_cup_indices):
            cup_idx = cup_indices[i]
            no_cup_idx = no_cup_indices[i]
            
            diff = np.abs(data[cup_idx] - data[no_cup_idx])
            axes[2, i].imshow(diff, cmap='hot')
            axes[2, i].set_title(f'Diff\nMax: {np.max(diff):.3f}', fontsize=8)
            axes[2, i].axis('off')
    
    plt.tight_layout()
    plt.show()

def train_advanced_classifier():
    """
    –û–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —Å transfer learning
    """
    print("üöÄ –ü–†–û–î–í–ò–ù–£–¢–û–ï –û–ë–£–ß–ï–ù–ò–ï –° TRANSFER LEARNING")
    print("="*70)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        data, labels, filenames = create_contrast_enhanced_dataset(
            images_dir="train",
            annotations_file="train/_annotations.coco.json",
            target_size=(224, 224),
            samples_per_class=150
        )
        
        if len(data) < 10:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        show_sample_images_detailed(data, labels, filenames)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        X_train, X_test, y_train, y_test = train_test_split(
            data, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
        print(f"  –û–±—É—á–µ–Ω–∏–µ: {X_train.shape[0]} ({Counter(y_train)})")
        print(f"  –í–∞–ª–∏–¥–∞—Ü–∏—è: {X_val.shape[0]} ({Counter(y_val)})")
        print(f"  –¢–µ—Å—Ç: {X_test.shape[0]} ({Counter(y_test)})")
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å transfer learning
        model = create_transfer_learning_model()
        
        # –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è –¥–ª—è –º–∞–ª–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        train_datagen = ImageDataGenerator(
            rotation_range=25,
            width_shift_range=0.15,
            height_shift_range=0.15,
            shear_range=0.15,
            zoom_range=0.15,
            horizontal_flip=True,
            vertical_flip=False,
            brightness_range=[0.8, 1.2],
            fill_mode='nearest'
        )
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ callbacks
        callbacks = [
            EarlyStopping(
                monitor='val_accuracy',
                patience=20,
                restore_best_weights=True,
                verbose=1,
                min_delta=0.005
            ),
            ModelCheckpoint(
                filepath='advanced_cup_classifier.h5',
                monitor='val_accuracy',
                mode='max',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.3,
                patience=8,
                min_lr=1e-8,
                verbose=1
            )
        ]
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        epochs = 100
        batch_size = 16  # –ú–∞–ª–µ–Ω—å–∫–∏–π batch –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        
        print(f"\nüéØ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:")
        print(f"  –ú–æ–¥–µ–ª—å: MobileNetV2 + Transfer Learning")
        print(f"  –≠–ø–æ—Ö–∏: {epochs}")
        print(f"  Batch size: {batch_size}")
        print(f"  Learning rate: 0.0001")
        print(f"  –ê—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è: –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è")
        
        # –û–±—É—á–µ–Ω–∏–µ
        print(f"\n{'='*50}")
        print("üéØ –ù–ê–ß–ò–ù–ê–ï–ú –û–ë–£–ß–ï–ù–ò–ï")
        print(f"{'='*50}")
        
        history = model.fit(
            train_datagen.flow(X_train, y_train, batch_size=batch_size),
            steps_per_epoch=max(1, len(X_train) // batch_size),
            validation_data=(X_val, y_val),
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à–∏–µ –≤–µ—Å–∞
        try:
            model.load_weights('advanced_cup_classifier.h5')
            print("\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –ª—É—á—à–∏–µ –≤–µ—Å–∞")
        except:
            print("\n‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –≤–µ—Å–∞")
        
        # Fine-tuning: —Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
        print("\nüî• FINE-TUNING: –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–∏")
        history_fine = None  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
        
        try:
            base_model = model.layers[0]
            base_model.trainable = True
            
            # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤—Å–µ —Å–ª–æ–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 20
            for layer in base_model.layers[:-20]:
                layer.trainable = False
            
            # –ü–µ—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º —Å –æ—á–µ–Ω—å –Ω–∏–∑–∫–∏–º learning rate
            model.compile(
                optimizer=Adam(learning_rate=0.00001),  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π LR –¥–ª—è fine-tuning
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy']
            )
            
            print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ—Å–ª–µ —Ä–∞–∑–º–æ—Ä–æ–∑–∫–∏: {sum([tf.keras.backend.count_params(w) for w in model.trainable_weights]):,}")
            
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å fine-tuning
            print("\nüî• –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å fine-tuning...")
            
            history_fine = model.fit(
                train_datagen.flow(X_train, y_train, batch_size=batch_size),
                steps_per_epoch=max(1, len(X_train) // batch_size),
                validation_data=(X_val, y_val),
                epochs=25,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö –¥–ª—è fine-tuning
                callbacks=callbacks,
                verbose=1,
                initial_epoch=len(history.history['accuracy'])
            )
            
            print("‚úÖ Fine-tuning –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ fine-tuning: {e}")
            print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª—å—é...")
            history_fine = None
        
        # –û—Ü–µ–Ω–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        print("\nüìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê")
        print("="*40)
        
        train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)
        val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
        test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)
        
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_acc:.4f}")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_acc:.4f}")
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_acc:.4f}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        test_predictions = model.predict(X_test, verbose=0)
        test_pred_classes = test_predictions.argmax(axis=1)
        
        # –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidences = test_predictions.max(axis=1)
        print(f"\nüéØ –ê–Ω–∞–ª–∏–∑ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:")
        print(f"  –°—Ä–µ–¥–Ω—è—è: {np.mean(confidences):.3f}")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞: {np.median(confidences):.3f}")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è: {np.min(confidences):.3f}")
        print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è: {np.max(confidences):.3f}")
        
        uncertain = np.sum(confidences < 0.7)
        print(f"  –ù–µ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π (<0.7): {uncertain}/{len(confidences)} ({uncertain/len(confidences)*100:.1f}%)")
        
        # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        print(f"\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
        class_names = ['no_cup', 'cup']
        report = classification_report(y_test, test_pred_classes, 
                                     target_names=class_names, digits=4)
        print(report)
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        cm = confusion_matrix(y_test, test_pred_classes)
        plt.figure(figsize=(10, 8))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names,
                    cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'})
        plt.title(f'–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫\n–¢–æ—á–Ω–æ—Å—Ç—å: {test_acc:.3f}, –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.mean(confidences):.3f}')
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
        plt.show()
        
        # –ì—Ä–∞—Ñ–∏–∫ –æ–±—É—á–µ–Ω–∏—è
        plt.figure(figsize=(15, 5))
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é –æ–±—É—á–µ–Ω–∏—è (–±–µ–∑–æ–ø–∞—Å–Ω–æ)
        all_accuracy = history.history['accuracy'][:]
        all_val_accuracy = history.history['val_accuracy'][:]
        all_loss = history.history['loss'][:]
        all_val_loss = history.history['val_loss'][:]
        
        # –î–æ–±–∞–≤–ª—è–µ–º fine-tuning –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if history_fine is not None and hasattr(history_fine, 'history'):
            all_accuracy.extend(history_fine.history.get('accuracy', []))
            all_val_accuracy.extend(history_fine.history.get('val_accuracy', []))
            all_loss.extend(history_fine.history.get('loss', []))
            all_val_loss.extend(history_fine.history.get('val_loss', []))
            fine_tuning_start = len(history.history['accuracy'])
        else:
            fine_tuning_start = None
        
        plt.subplot(1, 3, 1)
        plt.plot(all_accuracy, label='–û–±—É—á–µ–Ω–∏–µ')
        plt.plot(all_val_accuracy, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
        if fine_tuning_start is not None:
            plt.axvline(x=fine_tuning_start, color='red', linestyle='--', alpha=0.7, label='Fine-tuning start')
        plt.title('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        plt.legend()
        
        plt.subplot(1, 3, 2)
        plt.plot(all_loss, label='–û–±—É—á–µ–Ω–∏–µ')
        plt.plot(all_val_loss, label='–í–∞–ª–∏–¥–∞—Ü–∏—è')
        if fine_tuning_start is not None:
            plt.axvline(x=fine_tuning_start, color='red', linestyle='--', alpha=0.7, label='Fine-tuning start')
        plt.title('–ü–æ—Ç–µ—Ä–∏')
        plt.xlabel('–≠–ø–æ—Ö–∞')
        plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
        plt.legend()
        
        plt.subplot(1, 3, 3)
        plt.hist(confidences, bins=20, alpha=0.7, edgecolor='black')
        plt.axvline(x=np.mean(confidences), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {np.mean(confidences):.3f}')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
        plt.xlabel('–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        try:
            model.save('advanced_cup_classifier_final.h5')
            print(f"\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: advanced_cup_classifier_final.h5")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        print(f"\n" + "="*60)
        print("üèÜ –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´")
        print("="*60)
        
        if test_acc > 0.9:
            status = "–û–¢–õ–ò–ß–ù–û ‚úÖ"
        elif test_acc > 0.8:
            status = "–•–û–†–û–®–û ‚úÖ"
        elif test_acc > 0.7:
            status = "–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û ‚ö†Ô∏è"
        else:
            status = "–ù–ï–£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û ‚ùå"
        
        print(f"–°—Ç–∞—Ç—É—Å: {status}")
        print(f"–¢–µ—Å—Ç–æ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {test_acc:.4f}")
        print(f"–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.mean(confidences):.3f}")
        
        if test_acc > 0.8 and np.mean(confidences) > 0.8:
            print(f"\nüéâ –û–¢–õ–ò–ß–ù–´–ô –†–ï–ó–£–õ–¨–¢–ê–¢! –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
            print("üìÅ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ñ–∞–π–ª: advanced_cup_classifier_final.h5")
        elif test_acc > 0.7:
            print(f"\n‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç.")
        else:
            print(f"\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
            print("   - –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
            print("   - –°–æ–±—Ä–∞—Ç—å –±–æ–ª—å—à–µ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
            print("   - –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É")
        
        return model, history, test_acc
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return None, None, 0

def quick_test_on_sample(model_path='advanced_cup_classifier_final.h5', 
                        images_dir="train", num_test_images=10):
    """
    –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
    """
    print("üß™ –ë–´–°–¢–†–´–ô –¢–ï–°–¢ –ú–û–î–ï–õ–ò")
    print("="*40)
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model = tf.keras.models.load_model(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Ñ–∞–π–ª—ã
        all_files = []
        for ext in ['*.jpg', '*.jpeg', '*.png']:
            all_files.extend(glob(os.path.join(images_dir, ext)))
        
        test_files = np.random.choice(all_files, min(num_test_images, len(all_files)), replace=False)
        
        print(f"\nüéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_files)} —Å–ª—É—á–∞–π–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö:")
        
        for i, file_path in enumerate(test_files):
            filename = os.path.basename(file_path)
            
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            processed = enhanced_preprocessing(file_path)
            if processed is not None:
                # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                prediction = model.predict(np.expand_dims(processed, axis=0), verbose=0)
                predicted_class = prediction.argmax()
                confidence = prediction.max()
                
                class_name = "–∫—Ä—É–∂–∫–∞" if predicted_class == 1 else "–Ω–µ –∫—Ä—É–∂–∫–∞"
                print(f"  {i+1}. {filename[:30]:30} -> {class_name:10} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.3f})")
            else:
                print(f"  {i+1}. {filename[:30]:30} -> –û–®–ò–ë–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

def diagnose_training_problems():
    """
    –î–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –æ–±—É—á–µ–Ω–∏–µ–º
    """
    print("üîß –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ü–†–û–ë–õ–ï–ú –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*50)
    
    print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –ø–ª–æ—Ö–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:")
    print("\n1. üìä –ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏:")
    print("   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤")
    print("   - –°–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏")
    print("   - –ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
    print("   - –ù–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã")
    
    print("\n2. üß† –ü—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ–ª—å—é:")
    print("   - –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("   - –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π learning rate")
    print("   - –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ")
    
    print("\n3. ‚öôÔ∏è –ü—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π:")
    print("   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è")
    print("   - –ü–æ—Ç–µ—Ä—è –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ resize")
    print("   - –ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
    
    print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    print("   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ transfer learning (–∫–∞–∫ –≤ —ç—Ç–æ–º –∫–æ–¥–µ)")
    print("   2. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö")
    print("   3. –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö")
    print("   4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã")
    print("   5. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã")

if __name__ == "__main__":
    print("üöÄ –ü–†–û–î–í–ò–ù–£–¢–´–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† –ö–†–£–ñ–ï–ö")
    print("="*70)
    print("–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:")
    print("‚úÖ Transfer Learning —Å MobileNetV2")
    print("‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è ImageNet –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è")
    print("‚úÖ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö")
    print("‚úÖ Fine-tuning –¥–ª—è –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    print("‚úÖ –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—è")
    print("‚úÖ –î–µ—Ç–∞–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞")
    print("="*70)
    
    # –°–Ω–∞—á–∞–ª–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–±–ª–µ–º—ã
    diagnose_training_problems()
    
    print(f"\n{'='*50}")
    input("–ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –Ω–∞—á–∞–ª–∞ –æ–±—É—á–µ–Ω–∏—è...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
    model, history, accuracy = train_advanced_classifier()
    
    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–∏–ª–∞—Å—å, —Ç–µ—Å—Ç–∏—Ä—É–µ–º –µ—ë
    if accuracy > 0.6:
        print(f"\n{'='*50}")
        print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ò")
        quick_test_on_sample()
    
    print(f"\nüèÅ –ó–ê–í–ï–†–®–ï–ù–û!")
    if accuracy > 0.8:
        print("üéâ –û—Ç–ª–∏—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    elif accuracy > 0.7:
        print("‚úÖ –•–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç!")
    else:
        print("‚ùå –ù—É–∂–Ω—ã —É–ª—É—á—à–µ–Ω–∏—è...")

